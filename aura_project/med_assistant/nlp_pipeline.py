"""
Pipeline NLP pour AURA - Assistant M√©dical
Traitement automatique des observations m√©dicales avec int√©gration FastAPI
"""

import logging
from typing import Dict, Any, Optional
import json
import random
import os
import tempfile
from .api_client import fastapi_client

# Imports pour Whisper (fallback local)
try:
    import torch
    import torchaudio
    from transformers import WhisperProcessor, WhisperForConditionalGeneration
    import librosa
    import soundfile as sf
    WHISPER_AVAILABLE = True
except ImportError as e:
    WHISPER_AVAILABLE = False
    print(f"‚ö†Ô∏è Whisper non disponible: {e}")

logger = logging.getLogger(__name__)


class NLPPipeline:
    """
    Pipeline de traitement NLP pour les observations m√©dicales
    Int√®gre : transcription Whisper, classification via FastAPI, extraction d'entit√©s, r√©sum√©
    """
    
    def __init__(self):
        """Initialise la pipeline NLP avec FastAPI et Whisper en fallback"""
        self.models_loaded = False
        self.whisper_available = WHISPER_AVAILABLE
        self.device = "cuda" if torch.cuda.is_available() else "cpu" if WHISPER_AVAILABLE else "cpu"
        self.fastapi_available = False
        self.available_models = []
        self._load_models()
    
    def _load_models(self):
        """
        Charge les mod√®les NLP incluant Whisper et v√©rifie FastAPI
        """
        try:
            # V√©rifier la disponibilit√© de FastAPI
            self.fastapi_available = fastapi_client.is_api_available()
            if self.fastapi_available:
                self.available_models = fastapi_client.get_available_models()
                logger.info(f"‚úÖ FastAPI disponible avec {len(self.available_models)} mod√®les: {self.available_models}")
            else:
                logger.warning("‚ö†Ô∏è FastAPI non disponible, utilisation des mod√®les locaux")
            
            # Charger Whisper pour la transcription (local)
            if self.whisper_available:
                logger.info("üé§ Chargement du mod√®le Whisper...")
                
                self.whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
                self.whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
                self.whisper_model.to(self.device)
                
                logger.info(f"‚úÖ Whisper charg√© sur {self.device}")
            else:
                logger.warning("‚ö†Ô∏è Whisper non disponible, utilisation de la simulation")
            
            self.models_loaded = True
            logger.info("‚úÖ Pipeline NLP initialis√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement des mod√®les: {e}")
            self.models_loaded = False
            self.whisper_available = False
            self.fastapi_available = False
    
    def transcribe_audio(self, audio_file_path: str) -> Optional[str]:
        """
        Transcrit un fichier audio en texte avec Whisper
        
        Args:
            audio_file_path: Chemin vers le fichier audio
            
        Returns:
            Texte transcrit ou None en cas d'erreur
        """
        try:
            if not self.whisper_available or not self.models_loaded:
                logger.warning("‚ö†Ô∏è Whisper non disponible, utilisation de la simulation")
                return self._mock_transcription()
            
            logger.info(f"üé§ D√©but transcription de: {audio_file_path}")
            
            # V√©rifier que le fichier existe
            if not os.path.exists(audio_file_path):
                logger.error(f"‚ùå Fichier audio non trouv√©: {audio_file_path}")
                return None
            
            # Charger l'audio avec librosa (plus robuste que torchaudio)
            try:
                # Charger et convertir √† 16kHz mono
                waveform, sr = librosa.load(audio_file_path, sr=16000, mono=True)
                logger.info(f"üìä Audio charg√©: {len(waveform)} √©chantillons √† {sr}Hz")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement audio: {e}")
                # Fallback avec torchaudio
                try:
                    waveform, sr = torchaudio.load(audio_file_path)
                    # Convertir en mono si st√©r√©o
                    if waveform.shape[0] > 1:
                        waveform = torch.mean(waveform, dim=0)
                    else:
                        waveform = waveform.squeeze()
                    
                    # Resample √† 16kHz
                    if sr != 16000:
                        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
                        waveform = resampler(waveform)
                    
                    waveform = waveform.numpy()
                    
                except Exception as e2:
                    logger.error(f"‚ùå Erreur fallback torchaudio: {e2}")
                    return None
            
            # Pr√©parer les inputs pour Whisper
            inputs = self.whisper_processor(
                waveform, 
                sampling_rate=16000, 
                return_tensors="pt", 
                padding=True
            )
            
            # D√©placer sur le bon device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            logger.info("üß† G√©n√©ration de la transcription...")
            
            # G√©n√©rer la transcription
            with torch.no_grad():
                generated_ids = self.whisper_model.generate(
                    inputs["input_features"],
                    attention_mask=inputs.get("attention_mask", None),
                    language="fr",  # Forcer le fran√ßais
                    task="transcribe",
                    max_length=448,  # Limite raisonnable
                    num_beams=5,     # Am√©liorer la qualit√©
                    do_sample=False  # D√©terministe
                )
            
            # D√©coder la transcription
            transcription = self.whisper_processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True
            )[0]
            
            # Nettoyer la transcription
            transcription = transcription.strip()
            
            logger.info(f"‚úÖ Transcription r√©ussie: {len(transcription)} caract√®res")
            logger.info(f"üìÑ Aper√ßu: {transcription[:100]}...")
            
            return transcription
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la transcription Whisper: {e}")
            # Fallback vers simulation en cas d'erreur
            return self._mock_transcription()
    
    def classify_theme(self, text: str) -> Optional[str]:
        """
        Classifie le th√®me m√©dical via FastAPI ou fallback local
        
        Args:
            text: Texte √† classifier
            
        Returns:
            Th√®me classifi√© ou None en cas d'erreur
        """
        try:
            if self.fastapi_available and self.available_models:
                # Utiliser le premier mod√®le disponible pour la classification
                model_name = self.available_models[0]
                
                # Pr√©parer la question pour la classification
                classification_prompt = f"Classifiez ce texte m√©dical selon ces cat√©gories (cardio, psy, diabete, neuro, pneumo, gastro, ortho, dermato, general, autre): {text}"
                
                result = fastapi_client.process_text(model_name, classification_prompt)
                
                if result['success']:
                    response = result['response'].lower()
                    # Extraire la classification de la r√©ponse
                    for theme in ['cardio', 'psy', 'diabete', 'neuro', 'pneumo', 'gastro', 'ortho', 'dermato', 'general', 'autre']:
                        if theme in response:
                            logger.info(f"üè∑Ô∏è Classification via FastAPI: {theme}")
                            return theme
                    
                    # Si aucun th√®me sp√©cifique trouv√©, utiliser le fallback
                    return self._mock_classification(text)
                else:
                    logger.warning(f"‚ö†Ô∏è Erreur FastAPI classification: {result['error']}")
                    return self._mock_classification(text)
            else:
                return self._mock_classification(text)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la classification: {e}")
            return self._mock_classification(text)
    
    def extract_entities(self, text: str) -> Dict[str, Any]:
        """
        Extrait les entit√©s m√©dicales via FastAPI ou fallback local
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dictionnaire des entit√©s extraites
        """
        try:
            if self.fastapi_available and self.available_models:
                # Utiliser le premier mod√®le disponible pour l'extraction d'entit√©s
                model_name = self.available_models[0]
                
                # Pr√©parer la question pour l'extraction d'entit√©s
                entities_prompt = f"Extrayez les entit√©s m√©dicales de ce texte (maladies, m√©dicaments, anatomie, proc√©dures, examens): {text}"
                
                result = fastapi_client.process_text(model_name, entities_prompt)
                
                if result['success']:
                    # Parser la r√©ponse pour extraire les entit√©s
                    # Pour l'instant, utiliser le fallback mais logger la r√©ponse
                    logger.info(f"üîç R√©ponse entit√©s FastAPI: {result['response'][:100]}...")
                    return self._mock_entities(text)
                else:
                    logger.warning(f"‚ö†Ô∏è Erreur FastAPI entit√©s: {result['error']}")
                    return self._mock_entities(text)
            else:
                return self._mock_entities(text)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction d'entit√©s: {e}")
            return self._mock_entities(text)
    
    def generate_summary(self, text: str) -> Optional[str]:
        """
        G√©n√®re un r√©sum√© via FastAPI ou fallback local
        
        Args:
            text: Texte √† r√©sumer
            
        Returns:
            R√©sum√© g√©n√©r√© ou None en cas d'erreur
        """
        try:
            if self.fastapi_available and self.available_models:
                # Utiliser le premier mod√®le disponible pour le r√©sum√©
                model_name = self.available_models[0]
                
                # Pr√©parer la question pour le r√©sum√©
                summary_prompt = f"R√©sumez ce texte m√©dical en fran√ßais de mani√®re concise et professionnelle: {text}"
                
                result = fastapi_client.process_text(model_name, summary_prompt)
                
                if result['success']:
                    summary = result['response'].strip()
                    logger.info(f"üìÑ R√©sum√© g√©n√©r√© via FastAPI: {len(summary)} caract√®res")
                    return summary
                else:
                    logger.warning(f"‚ö†Ô∏è Erreur FastAPI r√©sum√©: {result['error']}")
                    return self._mock_summary(text)
            else:
                return self._mock_summary(text)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration du r√©sum√©: {e}")
            return self._mock_summary(text)
    
    def process_observation(self, observation) -> Dict[str, Any]:
        """
        Traite une observation compl√®te avec toute la pipeline
        
        Args:
            observation: Instance du mod√®le Observation
            
        Returns:
            Dictionnaire avec tous les r√©sultats du traitement
        """
        results = {
            'transcription': None,
            'theme_classe': None,
            'resume': None,
            'entites': {},
            'success': False,
            'error': None,
            'fastapi_used': self.fastapi_available
        }
        
        try:
            logger.info(f"üîÑ D√©but traitement observation {observation.id}")
            
            # 1. Transcription si fichier audio (toujours local avec Whisper)
            if observation.audio_file:
                logger.info(f"üé§ Transcription du fichier: {observation.audio_file.path}")
                transcription = self.transcribe_audio(observation.audio_file.path)
                if transcription:
                    results['transcription'] = transcription
                    logger.info("‚úÖ Transcription termin√©e")
                else:
                    logger.warning("‚ö†Ô∏è √âchec de la transcription")
            
            # 2. D√©terminer le texte source
            text_source = results['transcription'] or observation.texte_saisi
            
            if not text_source:
                results['error'] = "Aucun texte disponible pour le traitement"
                logger.error("‚ùå Aucun texte source disponible")
                return results
            
            logger.info(f"üìù Texte source: {len(text_source)} caract√®res")
            
            # 3. Classification du th√®me (FastAPI ou local)
            theme = self.classify_theme(text_source)
            if theme:
                results['theme_classe'] = theme
                logger.info(f"üè∑Ô∏è Th√®me classifi√©: {theme}")
            
            # 4. Extraction d'entit√©s (FastAPI ou local)
            entities = self.extract_entities(text_source)
            results['entites'] = entities
            logger.info(f"üîç Entit√©s extraites: {len(entities)} cat√©gories")
            
            # 5. G√©n√©ration du r√©sum√© (FastAPI ou local)
            summary = self.generate_summary(text_source)
            if summary:
                results['resume'] = summary
                logger.info("üìÑ R√©sum√© g√©n√©r√©")
            
            results['success'] = True
            logger.info(f"‚úÖ Traitement NLP termin√© pour l'observation {observation.id}")
            
        except Exception as e:
            error_msg = f"Erreur lors du traitement NLP: {e}"
            logger.error(f"‚ùå {error_msg}")
            results['error'] = error_msg
        
        return results
    
    def get_status(self) -> Dict[str, Any]:
        """
        Retourne le statut de la pipeline
        
        Returns:
            Dictionnaire avec le statut des diff√©rents composants
        """
        return {
            'whisper_available': self.whisper_available,
            'fastapi_available': self.fastapi_available,
            'available_models': self.available_models,
            'models_loaded': self.models_loaded,
            'device': self.device
        }
    
    # M√©thodes de simulation pour le d√©veloppement (inchang√©es)
    def _mock_transcription(self) -> str:
        """Simulation de transcription pour le d√©veloppement"""
        transcriptions = [
            "Patient pr√©sente des douleurs thoraciques depuis ce matin. Tension art√©rielle √©lev√©e √† 160/95. Prescrit un ECG et analyses sanguines.",
            "Consultation de suivi pour diab√®te de type 2. Glyc√©mie √† jeun √† 1,45 g/L. Ajustement de la metformine √† 1000mg matin et soir.",
            "Patient anxieux, troubles du sommeil depuis 3 semaines. Prescrit anxiolytique l√©ger et suivi psychologique.",
            "Douleur abdominale chronique, suspicion de gastrite. Prescription d'IPP et fibroscopie √† programmer."
        ]
        return random.choice(transcriptions)
    
    def _mock_classification(self, text: str) -> str:
        """Simulation de classification pour le d√©veloppement"""
        text_lower = text.lower()
        if any(word in text_lower for word in ['c≈ìur', 'cardiaque', 'tension', 'ecg', 'thoracique']):
            return 'cardio'
        elif any(word in text_lower for word in ['anxi√©t√©', 'd√©pression', 'stress', 'anxieux', 'sommeil']):
            return 'psy'
        elif any(word in text_lower for word in ['diab√®te', 'glyc√©mie', 'insuline', 'metformine']):
            return 'diabete'
        elif any(word in text_lower for word in ['abdomen', 'gastrite', 'estomac', 'digestif']):
            return 'gastro'
        else:
            return 'general'
    
    def _mock_entities(self, text: str) -> Dict[str, Any]:
        """Simulation d'extraction d'entit√©s avec nouvelles cat√©gories"""
        text_lower = text.lower()
        entities = {
            'DISO': [],  # Disorders
            'CHEM': [],  # Chemicals/Drugs
            'ANAT': [],  # Anatomy
            'PROC': [],  # Procedures
            'TEST': [],  # Tests
            'MED': [],   # Medications
            'BODY': []   # Body parts
        }
        
        # Simulation bas√©e sur le contenu
        if 'douleur' in text_lower:
            entities['DISO'].append('douleur thoracique')
        if 'diab√®te' in text_lower:
            entities['DISO'].append('diab√®te de type 2')
        if 'anxi√©t√©' in text_lower or 'anxieux' in text_lower:
            entities['DISO'].append('troubles anxieux')
        if 'gastrite' in text_lower:
            entities['DISO'].append('gastrite chronique')
            
        if 'metformine' in text_lower:
            entities['MED'].append('metformine 1000mg')
        if 'anxiolytique' in text_lower:
            entities['MED'].append('anxiolytique')
        if 'ipp' in text_lower:
            entities['MED'].append('inhibiteur de pompe √† protons')
            
        if 'thorax' in text_lower or 'thoracique' in text_lower:
            entities['ANAT'].append('thorax')
        if 'c≈ìur' in text_lower:
            entities['ANAT'].append('c≈ìur')
        if 'abdomen' in text_lower:
            entities['ANAT'].append('abdomen')
            
        if 'ecg' in text_lower:
            entities['TEST'].append('√©lectrocardiogramme')
        if 'glyc√©mie' in text_lower:
            entities['TEST'].append('glyc√©mie √† jeun')
        if 'analyses sanguines' in text_lower:
            entities['TEST'].append('bilan sanguin')
        if 'fibroscopie' in text_lower:
            entities['PROC'].append('fibroscopie gastrique')
            
        # Nettoyer les listes vides
        return {k: v for k, v in entities.items() if v}
    
    def _mock_summary(self, text: str) -> str:
        """Simulation de r√©sum√© pour le d√©veloppement"""
        summaries = {
            'cardio': "Consultation cardiologique : douleurs thoraciques avec HTA. Examens compl√©mentaires prescrits.",
            'diabete': "Suivi diab√©tologique : ajustement th√©rapeutique suite √† d√©s√©quilibre glyc√©mique.",
            'psy': "Consultation psychiatrique : troubles anxieux avec retentissement sur le sommeil. Traitement initi√©.",
            'gastro': "Consultation gastroent√©rologique : douleurs abdominales chroniques. Explorations √† poursuivre.",
            'general': "Consultation de m√©decine g√©n√©rale : prise en charge symptomatique et suivi."
        }
        
        # D√©terminer le type bas√© sur le texte
        text_lower = text.lower()
        if any(word in text_lower for word in ['c≈ìur', 'cardiaque', 'tension', 'ecg']):
            return summaries['cardio']
        elif any(word in text_lower for word in ['diab√®te', 'glyc√©mie', 'metformine']):
            return summaries['diabete']
        elif any(word in text_lower for word in ['anxi√©t√©', 'anxieux', 'sommeil']):
            return summaries['psy']
        elif any(word in text_lower for word in ['abdomen', 'gastrite']):
            return summaries['gastro']
        else:
            return summaries['general']


# Instance globale de la pipeline
nlp_pipeline = NLPPipeline()


def process_observation_async(observation_id: int):
    """
    Fonction pour traiter une observation de mani√®re asynchrone
    √Ä utiliser avec Celery ou un syst√®me de t√¢ches similaire
    """
    from .models import Observation
    
    try:
        observation = Observation.objects.get(id=observation_id)
        results = nlp_pipeline.process_observation(observation)
        
        # Mise √† jour de l'observation avec les r√©sultats
        if results['success']:
            observation.transcription = results['transcription']
            observation.theme_classe = results['theme_classe']
            observation.resume = results['resume']
            observation.entites = results['entites']
            observation.traitement_termine = True
        else:
            observation.traitement_erreur = results['error']
        
        observation.save()
        
    except Observation.DoesNotExist:
        logger.error(f"Observation {observation_id} non trouv√©e")
    except Exception as e:
        logger.error(f"Erreur lors du traitement asynchrone: {e}")