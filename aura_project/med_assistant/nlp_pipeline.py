"""
Pipeline NLP pour AURA - Assistant M√©dical
Traitement automatique des observations m√©dicales avec mod√®les Hugging Face directs
Optimis√© pour GPU avec m√©moire limit√©e
"""

import logging
from typing import Dict, Any, Optional, List, Tuple
import json
import os
import tempfile

# Imports pour Whisper (transcription)
try:
    import torch
    import torchaudio
    from transformers import WhisperProcessor, WhisperForConditionalGeneration
    import librosa
    import soundfile as sf
    WHISPER_AVAILABLE = True
except ImportError as e:
    WHISPER_AVAILABLE = False
    print(f"‚ö†Ô∏è Whisper non disponible: {e}")

# Imports pour les mod√®les Hugging Face
try:
    from transformers import (
        AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification,
        T5Tokenizer, T5ForConditionalGeneration, pipeline
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"‚ö†Ô∏è Transformers non disponible: {e}")

logger = logging.getLogger(__name__)


class NLPPipeline:
    """
    Pipeline de traitement NLP pour les observations m√©dicales
    Int√®gre : transcription Whisper, classification, extraction d'entit√©s DrBERT-CASM2, r√©sum√© T5
    Optimis√© pour GPU avec m√©moire limit√©e
    """
    
    def __init__(self):
        """Initialise la pipeline NLP avec les mod√®les Hugging Face directs"""
        self.models_loaded = False
        self.whisper_available = WHISPER_AVAILABLE
        self.transformers_available = TRANSFORMERS_AVAILABLE
        self.device = "cuda" if torch.cuda.is_available() else "cpu" if WHISPER_AVAILABLE else "cpu"
        
        # Mod√®les charg√©s √† la demande pour √©conomiser la m√©moire
        self.whisper_model = None
        self.whisper_processor = None
        self.classification_model = None
        self.classification_tokenizer = None
        self.drbert_pipeline = None
        self.t5_pipeline = None
        
        # Configuration des mod√®les - NOUVEAU DrBERT-CASM2
        self.models_config = {
            'classification': 'waelbensoltana/finetuned-medical-fr',
            'entities': 'medkit/DrBERT-CASM2',  # NOUVEAU MOD√àLE !
            'summarization': 'plguillou/t5-base-fr-sum-cnndm'
        }
        
        # Mapping des pathologies du mod√®le
        self.pathology_mapping = {
            0: 'cardiovasculaire',
            1: 'psy', 
            2: 'diabete'
        }
        
        # Mapping des entit√©s DrBERT-CASM2 vers nos cat√©gories
        self.drbert_entity_mapping = {
            'DISO': 'DISO',  # Disorders/Maladies
            'CHEM': 'CHEM',  # Chemicals/M√©dicaments
            'ANAT': 'ANAT',  # Anatomie
            'PROC': 'PROC',  # Proc√©dures
            'LIVB': 'ANAT',  # Living beings -> Anatomie
            'OBJC': 'PROC',  # Objects -> Proc√©dures
            'PHEN': 'DISO',  # Phenomena -> Disorders
            'PHYS': 'DISO',  # Physiology -> Disorders
            'GEOG': 'PROC',  # Geography -> Proc√©dures
            'CONC': 'PROC',  # Concepts -> Proc√©dures
        }
        
        self._load_models()
    
    def _clear_gpu_cache(self):
        """Nettoie le cache GPU pour lib√©rer de la m√©moire"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("üßπ Cache GPU nettoy√©")
    
    def _load_models(self):
        """
        Charge les mod√®les NLP incluant Whisper
        Optimis√© pour GPU avec m√©moire limit√©e
        """
        try:
            # Charger Whisper pour la transcription (local) - PRIORIT√â
            if self.whisper_available:
                logger.info("üé§ Chargement du mod√®le Whisper...")
                
                self.whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
                self.whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
                self.whisper_model.to(self.device)
                
                logger.info(f"‚úÖ Whisper charg√© sur {self.device}")
                
                # Afficher l'utilisation m√©moire apr√®s Whisper
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    logger.info(f"üìä M√©moire GPU apr√®s Whisper: {memory_used:.2f}GB / {memory_total:.2f}GB")
            else:
                logger.warning("‚ö†Ô∏è Whisper non disponible")
            
            # Les autres mod√®les seront charg√©s √† la demande pour √©conomiser la m√©moire
            logger.info("üí° Classification, DrBERT-CASM2 et T5 seront charg√©s √† la demande pour optimiser la m√©moire")
            
            self.models_loaded = True
            logger.info("‚úÖ Pipeline NLP initialis√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement des mod√®les: {e}")
            self.models_loaded = False
            self.whisper_available = False
            self.transformers_available = False
    
    def _load_classification_on_demand(self):
        """Charge le mod√®le de classification √† la demande"""
        if self.classification_model is not None:
            return True
            
        if not self.transformers_available:
            return False
            
        try:
            logger.info("üè∑Ô∏è Chargement du mod√®le de classification √† la demande...")
            
            self.classification_tokenizer = AutoTokenizer.from_pretrained(self.models_config['classification'])
            self.classification_model = AutoModelForSequenceClassification.from_pretrained(
                self.models_config['classification'],
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                low_cpu_mem_usage=True
            )
            
            if torch.cuda.is_available():
                self.classification_model.to(self.device)
            
            logger.info(f"‚úÖ Mod√®le de classification charg√© sur {self.device}")
            
            # Afficher l'utilisation m√©moire
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"üìä M√©moire GPU apr√®s classification: {memory_used:.2f}GB / {memory_total:.2f}GB")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®le de classification: {e}")
            self.transformers_available = False
            return False
    
    def _load_drbert_on_demand(self):
        """Charge DrBERT-CASM2 √† la demande et lib√®re le mod√®le de classification si n√©cessaire"""
        if self.drbert_pipeline is not None:
            return True
            
        if not self.transformers_available:
            return False
            
        try:
            logger.info("üß† Chargement du mod√®le DrBERT-CASM2 √† la demande...")
            
            # Lib√©rer le mod√®le de classification temporairement si n√©cessaire
            classification_was_loaded = self.classification_model is not None
            if classification_was_loaded and torch.cuda.is_available():
                logger.info("üîÑ Lib√©ration temporaire du mod√®le de classification pour DrBERT-CASM2...")
                del self.classification_model
                del self.classification_tokenizer
                self.classification_model = None
                self.classification_tokenizer = None
                self._clear_gpu_cache()
            
            # Charger DrBERT-CASM2 avec optimisations m√©moire
            tokenizer = AutoTokenizer.from_pretrained(self.models_config['entities'])
            model = AutoModelForTokenClassification.from_pretrained(
                self.models_config['entities'],
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                low_cpu_mem_usage=True
            )
            
            # Cr√©er le pipeline NER avec aggregation_strategy="simple" comme dans votre exemple
            self.drbert_pipeline = pipeline(
                "ner",
                model=model,
                tokenizer=tokenizer,
                aggregation_strategy="simple",
                device=0 if torch.cuda.is_available() else -1,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            logger.info(f"‚úÖ DrBERT-CASM2 charg√© sur {self.device}")
            
            # Afficher l'utilisation m√©moire
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"üìä M√©moire GPU apr√®s DrBERT-CASM2: {memory_used:.2f}GB / {memory_total:.2f}GB")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement DrBERT-CASM2: {e}")
            self.transformers_available = False
            
            # Recharger le mod√®le de classification si il √©tait charg√©
            if classification_was_loaded:
                self._load_classification_on_demand()
            
            return False
    
    def _load_t5_on_demand(self):
        """Charge T5 √† la demande et lib√®re d'autres mod√®les si n√©cessaire"""
        if self.t5_pipeline is not None:
            return True
            
        if not self.transformers_available:
            return False
            
        try:
            logger.info("üìù Chargement du mod√®le T5 √† la demande...")
            
            # Lib√©rer DrBERT temporairement si n√©cessaire
            drbert_was_loaded = self.drbert_pipeline is not None
            if drbert_was_loaded:
                logger.info("üîÑ Lib√©ration temporaire de DrBERT-CASM2 pour T5...")
                del self.drbert_pipeline
                self.drbert_pipeline = None
                self._clear_gpu_cache()
            
            # Charger T5 avec optimisations m√©moire
            tokenizer = T5Tokenizer.from_pretrained(self.models_config['summarization'], legacy=False)
            model = T5ForConditionalGeneration.from_pretrained(
                self.models_config['summarization'],
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                low_cpu_mem_usage=True
            )
            
            # Cr√©er le pipeline de r√©sum√© avec optimisations
            self.t5_pipeline = pipeline(
                "summarization",
                model=model,
                tokenizer=tokenizer,
                device=0 if torch.cuda.is_available() else -1,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            logger.info(f"‚úÖ T5 r√©sum√© charg√© sur {self.device}")
            
            # Afficher l'utilisation m√©moire
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"üìä M√©moire GPU apr√®s T5: {memory_used:.2f}GB / {memory_total:.2f}GB")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement T5: {e}")
            self.transformers_available = False
            return False
    
    def _reload_whisper(self):
        """Recharge Whisper si n√©cessaire"""
        if self.whisper_model is not None or not self.whisper_available:
            return
            
        try:
            logger.info("üîÑ Rechargement de Whisper...")
            self.whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
            self.whisper_model.to(self.device)
            logger.info("‚úÖ Whisper recharg√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur rechargement Whisper: {e}")
    
    def clean_entities(self, entities: List[Dict]) -> List[Dict]:
        """
        Votre fonction clean_entities exactement comme vous l'avez √©crite !
        Post-traitement pour nettoyer les entit√©s DrBERT-CASM2
        
        Args:
            entities: Liste des entit√©s brutes de DrBERT-CASM2
            
        Returns:
            Liste des entit√©s nettoy√©es
        """
        cleaned = []
        for ent in entities:
            word = ent["word"].replace("##", "")
            # On fusionne si c'est la suite d'un mot pr√©c√©dent
            if cleaned and ent["start"] == cleaned[-1]["end"]:
                cleaned[-1]["word"] += word
                cleaned[-1]["end"] = ent["end"]
            else:
                cleaned.append({
                    "entity_group": ent["entity_group"],
                    "word": word,
                    "start": ent["start"],
                    "end": ent["end"],
                    "score": ent["score"]
                })
        return cleaned
    
    def transcribe_audio(self, audio_file_path: str) -> Optional[str]:
        """
        Transcrit un fichier audio en texte avec Whisper
        
        Args:
            audio_file_path: Chemin vers le fichier audio
            
        Returns:
            Texte transcrit ou None en cas d'erreur
        """
        try:
            if not self.whisper_available or not self.models_loaded:
                logger.warning("‚ö†Ô∏è Whisper non disponible")
                return None
            
            # S'assurer que Whisper est charg√©
            if self.whisper_model is None:
                self._reload_whisper()
                if self.whisper_model is None:
                    return None
            
            logger.info(f"üé§ D√©but transcription de: {audio_file_path}")
            
            # V√©rifier que le fichier existe
            if not os.path.exists(audio_file_path):
                logger.error(f"‚ùå Fichier audio non trouv√©: {audio_file_path}")
                return None
            
            # Charger l'audio avec librosa (plus robuste que torchaudio)
            try:
                # Charger et convertir √† 16kHz mono
                waveform, sr = librosa.load(audio_file_path, sr=16000, mono=True)
                logger.info(f"üìä Audio charg√©: {len(waveform)} √©chantillons √† {sr}Hz")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement audio: {e}")
                # Fallback avec torchaudio
                try:
                    waveform, sr = torchaudio.load(audio_file_path)
                    # Convertir en mono si st√©r√©o
                    if waveform.shape[0] > 1:
                        waveform = torch.mean(waveform, dim=0)
                    else:
                        waveform = waveform.squeeze()
                    
                    # Resample √† 16kHz
                    if sr != 16000:
                        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
                        waveform = resampler(waveform)
                    
                    waveform = waveform.numpy()
                    
                except Exception as e2:
                    logger.error(f"‚ùå Erreur fallback torchaudio: {e2}")
                    return None
            
            # Pr√©parer les inputs pour Whisper
            inputs = self.whisper_processor(
                waveform, 
                sampling_rate=16000, 
                return_tensors="pt", 
                padding=True
            )
            
            # D√©placer sur le bon device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            logger.info("üß† G√©n√©ration de la transcription...")
            
            # G√©n√©rer la transcription
            with torch.no_grad():
                generated_ids = self.whisper_model.generate(
                    inputs["input_features"],
                    attention_mask=inputs.get("attention_mask", None),
                    language="fr",  # Forcer le fran√ßais
                    task="transcribe",
                    max_length=448,  # Limite raisonnable
                    num_beams=5,     # Am√©liorer la qualit√©
                    do_sample=False  # D√©terministe
                )
            
            # D√©coder la transcription
            transcription = self.whisper_processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True
            )[0]
            
            # Nettoyer la transcription
            transcription = transcription.strip()
            
            logger.info(f"‚úÖ Transcription r√©ussie: {len(transcription)} caract√®res")
            logger.info(f"üìÑ Aper√ßu: {transcription[:100]}...")
            
            return transcription
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la transcription Whisper: {e}")
            return None
    
    def classify_theme(self, text: str) -> tuple[Optional[str], Optional[int]]:
        """
        Classifie le th√®me m√©dical avec le mod√®le waelbensoltana/finetuned-medical-fr
        
        Args:
            text: Texte √† classifier
            
        Returns:
            Tuple (th√®me_classifi√©, pr√©diction_num√©rique) ou (None, None) en cas d'erreur
        """
        try:
            # Charger le mod√®le de classification √† la demande
            if not self._load_classification_on_demand():
                logger.warning("‚ö†Ô∏è Mod√®le de classification non disponible")
                return None, None
            
            logger.info(f"üè∑Ô∏è Classification du texte: {text[:50]}...")
            
            # Tokeniser le texte
            inputs = self.classification_tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                padding=True,
                max_length=512
            )
            
            # D√©placer sur le bon device si n√©cessaire
            if torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Pr√©diction
            with torch.no_grad():
                outputs = self.classification_model(**inputs)
                prediction = torch.argmax(outputs.logits, dim=1).item()
            
            # Convertir la pr√©diction en th√®me
            theme = self.pathology_mapping.get(prediction, 'autre')
            
            logger.info(f"‚úÖ Classification: pr√©diction={prediction}, th√®me={theme}")
            
            # Lib√©rer le mod√®le de classification apr√®s utilisation pour √©conomiser la m√©moire
            if self.classification_model is not None:
                logger.info("üîÑ Lib√©ration du mod√®le de classification apr√®s utilisation")
                del self.classification_model
                del self.classification_tokenizer
                self.classification_model = None
                self.classification_tokenizer = None
                self._clear_gpu_cache()
            
            return theme, prediction
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la classification: {e}")
            return None, None
    
    def extract_entities_drbert(self, text: str) -> Dict[str, List[str]]:
        """
        Extrait les entit√©s m√©dicales avec DrBERT-CASM2 en utilisant votre fonction clean_entities
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dictionnaire des entit√©s extraites par cat√©gorie
        """
        try:
            # Charger DrBERT-CASM2 √† la demande
            if not self._load_drbert_on_demand():
                logger.warning("‚ö†Ô∏è DrBERT-CASM2 non disponible")
                return {}
            
            logger.info(f"üîç Extraction d'entit√©s DrBERT-CASM2 pour: {text[:50]}...")
            
            # Utiliser le pipeline NER de DrBERT-CASM2 avec aggregation_strategy="simple"
            entities = self.drbert_pipeline(text)
            
            logger.info(f"üîç DrBERT-CASM2 a trouv√© {len(entities)} entit√©s brutes")
            
            # DEBUG: Afficher quelques entit√©s brutes
            for i, ent in enumerate(entities[:5]):
                logger.info(f"  Entit√© brute {i}: {ent}")
            
            # Appliquer votre fonction clean_entities
            cleaned_entities = self.clean_entities(entities)
            
            logger.info(f"üßπ Apr√®s clean_entities: {len(cleaned_entities)} entit√©s nettoy√©es")
            
            # DEBUG: Afficher quelques entit√©s nettoy√©es
            for i, ent in enumerate(cleaned_entities[:5]):
                logger.info(f"  Entit√© nettoy√©e {i}: {ent}")
            
            # Organiser les entit√©s par cat√©gorie
            categorized_entities = {
                'DISO': [],  # Disorders
                'CHEM': [],  # Chemicals/Drugs
                'ANAT': [],  # Anatomy
                'PROC': [],  # Procedures
            }
            
            for entity in cleaned_entities:
                entity_label = entity['entity_group']
                entity_text = entity['word'].strip()
                confidence = entity['score']
                
                # Mapper vers nos cat√©gories
                mapped_category = self.drbert_entity_mapping.get(entity_label, 'PROC')
                
                # Filtrer par confiance (seuil √† 0.5) et longueur minimale
                if confidence > 0.5 and len(entity_text) > 2:
                    # √âviter les doublons
                    if entity_text not in categorized_entities[mapped_category]:
                        categorized_entities[mapped_category].append(entity_text)
                        logger.debug(f"  ‚úì {mapped_category}: {entity_text} (conf: {confidence:.2f})")
            
            # Nettoyer les cat√©gories vides
            categorized_entities = {k: v for k, v in categorized_entities.items() if v}
            
            logger.info(f"‚úÖ DrBERT-CASM2: {sum(len(v) for v in categorized_entities.values())} entit√©s extraites et nettoy√©es")
            
            # DEBUG: Afficher le r√©sultat final
            for category, entities_list in categorized_entities.items():
                logger.info(f"  {category}: {entities_list}")
            
            # Lib√©rer DrBERT apr√®s utilisation pour √©conomiser la m√©moire
            if self.drbert_pipeline is not None:
                logger.info("üîÑ Lib√©ration de DrBERT-CASM2 apr√®s utilisation")
                del self.drbert_pipeline
                self.drbert_pipeline = None
                self._clear_gpu_cache()
            
            return categorized_entities
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction DrBERT-CASM2: {e}")
            logger.exception("D√©tails de l'erreur:")
            return {}
    
    def extract_entities(self, text: str) -> Dict[str, Any]:
        """
        Extrait les entit√©s m√©dicales (utilise DrBERT-CASM2 maintenant)
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dictionnaire des entit√©s extraites
        """
        return self.extract_entities_drbert(text)
    
    def generate_summary_t5(self, text: str) -> Optional[str]:
        """
        G√©n√®re un r√©sum√© avec T5 fran√ßais (charg√© √† la demande)
        
        Args:
            text: Texte √† r√©sumer
            
        Returns:
            R√©sum√© g√©n√©r√© ou None en cas d'erreur
        """
        try:
            # Charger T5 √† la demande
            if not self._load_t5_on_demand():
                logger.warning("‚ö†Ô∏è T5 non disponible")
                return None
            
            logger.info(f"üìù G√©n√©ration r√©sum√© T5 pour: {text[:50]}...")
            
            # Utiliser le pipeline de r√©sum√© T5
            summary = self.t5_pipeline(
                text, 
                max_length=100, 
                min_length=20, 
                do_sample=False
            )[0]['summary_text']
            
            logger.info(f"‚úÖ T5: R√©sum√© g√©n√©r√© ({len(summary)} caract√®res)")
            
            # Lib√©rer T5 apr√®s utilisation pour √©conomiser la m√©moire
            if self.t5_pipeline is not None:
                logger.info("üîÑ Lib√©ration de T5 apr√®s utilisation")
                del self.t5_pipeline
                self.t5_pipeline = None
                self._clear_gpu_cache()
            
            return summary.strip()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration T5: {e}")
            return None
    
    def generate_summary(self, text: str) -> Optional[str]:
        """
        G√©n√®re un r√©sum√© via T5 local
        
        Args:
            text: Texte √† r√©sumer
            
        Returns:
            R√©sum√© g√©n√©r√© ou None en cas d'erreur
        """
        return self.generate_summary_t5(text)
    
    def process_observation(self, observation) -> Dict[str, Any]:
        """
        Traite une observation compl√®te avec toute la pipeline
        
        Args:
            observation: Instance du mod√®le Observation
            
        Returns:
            Dictionnaire avec tous les r√©sultats du traitement
        """
        results = {
            'transcription': None,
            'theme_classe': None,
            'model_prediction': None,
            'resume': None,
            'entites': {},
            'success': False,
            'error': None,
            'classification_used': self.transformers_available,
            'drbert_used': self.transformers_available,
            't5_used': self.transformers_available
        }
        
        try:
            logger.info(f"üîÑ D√©but traitement observation {observation.id}")
            
            # 1. Transcription si fichier audio (toujours local avec Whisper)
            if observation.audio_file:
                logger.info(f"üé§ Transcription du fichier: {observation.audio_file.path}")
                transcription = self.transcribe_audio(observation.audio_file.path)
                if transcription:
                    results['transcription'] = transcription
                    logger.info("‚úÖ Transcription termin√©e")
                else:
                    logger.warning("‚ö†Ô∏è √âchec de la transcription")
            
            # 2. D√©terminer le texte source
            text_source = results['transcription'] or observation.texte_saisi
            
            if not text_source:
                results['error'] = "Aucun texte disponible pour le traitement"
                logger.error("‚ùå Aucun texte source disponible")
                return results
            
            logger.info(f"üìù Texte source: {len(text_source)} caract√®res")
            
            # 3. Classification du th√®me avec pr√©diction num√©rique
            theme, prediction = self.classify_theme(text_source)
            if theme:
                results['theme_classe'] = theme
                results['model_prediction'] = prediction
                logger.info(f"üè∑Ô∏è Th√®me classifi√©: {theme} (pr√©diction: {prediction})")
            
            # 4. Extraction d'entit√©s (DrBERT-CASM2 avec votre fonction clean_entities)
            entities = self.extract_entities(text_source)
            results['entites'] = entities
            logger.info(f"üîç Entit√©s extraites avec clean_entities: {len(entities)} cat√©gories")
            
            # 5. G√©n√©ration du r√©sum√© (T5 local √† la demande)
            summary = self.generate_summary(text_source)
            if summary:
                results['resume'] = summary
                logger.info("üìÑ R√©sum√© g√©n√©r√©")
            
            results['success'] = True
            logger.info(f"‚úÖ Traitement NLP termin√© pour l'observation {observation.id}")
            
        except Exception as e:
            error_msg = f"Erreur lors du traitement NLP: {e}"
            logger.error(f"‚ùå {error_msg}")
            results['error'] = error_msg
        
        return results
    
    def get_status(self) -> Dict[str, Any]:
        """
        Retourne le statut de la pipeline
        
        Returns:
            Dictionnaire avec le statut des diff√©rents composants
        """
        return {
            'whisper_available': self.whisper_available,
            'drbert_available': self.transformers_available,
            't5_available': self.transformers_available,
            'classification_available': self.transformers_available,
            'available_models': [
                'waelbensoltana/finetuned-medical-fr',
                'medkit/DrBERT-CASM2',  # NOUVEAU MOD√àLE !
                'plguillou/t5-base-fr-sum-cnndm'
            ] if self.transformers_available else [],
            'models_loaded': self.models_loaded,
            'device': self.device,
            'pathology_mapping': self.pathology_mapping,
            'drbert_entity_mapping': self.drbert_entity_mapping,
            'memory_optimized': True,
            'models_config': self.models_config
        }


# Instance globale de la pipeline
nlp_pipeline = NLPPipeline()


def process_observation_async(observation_id: int):
    """
    Fonction pour traiter une observation de mani√®re asynchrone
    √Ä utiliser avec Celery ou un syst√®me de t√¢ches similaire
    """
    from .models import Observation
    
    try:
        observation = Observation.objects.get(id=observation_id)
        results = nlp_pipeline.process_observation(observation)
        
        # Mise √† jour de l'observation avec les r√©sultats
        if results['success']:
            observation.transcription = results['transcription']
            observation.theme_classe = results['theme_classe']
            observation.model_prediction = results['model_prediction']
            observation.resume = results['resume']
            observation.entites = results['entites']
            observation.traitement_termine = True
        else:
            observation.traitement_erreur = results['error']
        
        observation.save()
        
    except Observation.DoesNotExist:
        logger.error(f"Observation {observation_id} non trouv√©e")
    except Exception as e:
        logger.error(f"Erreur lors du traitement asynchrone: {e}")